<!doctype html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="SVLTA: Benchmarking Vision-Language Temporal Alignment via Synthetic Video Situations">

    <title>SVLTA: Benchmarking Vision-Language Temporal Alignment via Synthetic Video Situations</title>

    <!-- font -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,100;0,200;0,300;0,400;0,600;0,700;1,100;1,200;1,300;1,400;1,600;1,700&display=swap" rel="stylesheet">
    <!--<link href="https://fonts.googleapis.com/css2?family=Mulish:ital,wght@0,200;0,400;0,800;0,900;1,200;1,400;1,800;1,900&display=swap" rel="stylesheet">-->

    <!-- link to CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC" crossorigin="anonymous">
    <link rel="stylesheet" href="css/style.css">

    <!-- Favicon -->
    <link rel="icon" type="image/png" href="https://mitibmwatsonailab.mit.edu/wp-content/themes/ibm-mit/assets/favicon/favicon-32x32.png" sizes="32x32">

    <!-- jQuery CSS/Js -->
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js" integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI" crossorigin="anonymous"></script>

    <!-- fontawesome for icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

    <!-- D3 -->
    <script src="https://d3js.org/d3.v6.min.js"></script>

</head>

<body>
    <div class="background"></div>
    <div class="news">News: The <a href="https://svlta-bench.github.io/SVLTA/" target="_blank">SVLTA Website</a> is available now.</div>
    <div class="header">
        <h1><span><b>SVLTA</b><br>Benchmarking Vision-Language Temporal Alignment via Synthetic Video Situations</span></h1>
        <div class="nav">
            <a href="#whatis">SVLTA</a>
            <a href="#preview">Preview</a>
            <a href="#generation">Generation Process</a>
            <a href="#demo">Data Examples</a>
            <a href="#repo">Download</a>
            <!--<a href="#paper">Paper</a>-->
            <!--<a href="#author">Author</a>-->
            <!--<a href="#Team">Team</a>-->
        </div>
    </div>
    <div class="content">

        <div id="whatis" class="section">
            <h4>SVLTA</h4>
            <p class="boxed">
                Our work aims to investigate and evaluate the ability of models to achieve alignment from a temporal perspective, specifically within more balanced temporal distributions, high-quality temporal annotations, and synthetic video situations.
                <b>SVLTA</b> consists of 96 different compositional actions, 26.2K synthetic video situations, and 78K high-quality temporal annotations with consistent visual-language semantics. This benchmark can provide a diagnostic evaluations in temporal question answering, distributional shift sensitiveness, and temporal alignment adaptation.
                <!--
                Our work aims to delve deeper into reasoning evaluations, specifically within dynamic, open-world, and structured context knowledge. 
                <b>SOK-Bench</b> consists of 44K questions and 10K situations with instance-level annotations depicted in the videos. The reasoning process is required to understand and apply situated knowledge and general knowledge for problem-solving.
                -->
            </p>
            <br><br>
            <!--
            <p class="boxed" style="background: none; border: solid 1px var(--green)">
                Welcome to evaluate your models on the <a class="download" href="https:" target="_blank">SOK-Bench Evaluation</a> and check results on the <a class="download" href="https://XX" target="_blank">SOK-Bench Challenge Leaderboard</a>.
            </p>
            -->
        </div>

        <div id="preview" class="section">
            <h4>Preview</h4>
            <p><b>SVLTA</b> consists of 96 different compositional actions, 26.2K synthetic video situations, and 78K high-quality temporal annotations with consistent visual-language semantics. This benchmark can provide a diagnostic evaluations in temporal question answering, distributional shift sensitiveness, and temporal alignment adaptation.
            <!--44K questions and 10K situations with instance-level annotations depicted in the videos. The reasoning process is required to understand and apply situated knowledge and general knowledge for problem-solving.</p>-->
            <img src="imgs/svlta_overview.png" width="100%">
        </div>

        <div id="generation" class="section">
            <h4>Generation Process</h4>
            <!--<p><b>SVLTA</b> consists of 96 different compositional actions, 26.2K synthetic video situations, and 78K high-quality temporal annotations with consistent visual-language semantics. This benchmark can provide a diagnostic evaluations in temporal question answering, distributional shift sensitiveness, and temporal alignment adaptation.-->
            <!--44K questions and 10K situations with instance-level annotations depicted in the videos. The reasoning process is required to understand and apply situated knowledge and general knowledge for problem-solving.</p>-->
            The benchmark generation process mainly contains five stages, including (a): Situation Component Initialization stage defines a series of compositional elements, which comprises diverse actions,
            agents, and situations, (b): Commonsense Activity Graph stage first builds a graph on the activity
            commonsense and then use the traversal algorithm and re-weighting sampling to acquire various and
            meaningful logical action chains, (c): Controllable Activity Manuscript stage operates the actions
            in logical action chains through different framerates and permutations to obtain the final activity
            manuscript, thereby balancing the temporal distribution, (d): Synthetic Video and Language Sentence
            Generation stages convert the generated activity manuscript to the functional programs and utilize it
            to generate synthetic videos and sentences, and (e): Visual-Language Temporal Alignment stage automatically associates the timestamps with the action in the sentence to obtain high-quality annotations.
            <img src="imgs/svlta_generation_latest.png" width="100%">
        </div>


        <div id="demo" class="section">
            <style>
                .video-container {
                    display: flex;
                    gap: 20px; /* 设置视频之间的距离 */
                }
                .video-item {
                    text-align: center; /* 使文字居中对齐 */
                }
                .caption {
                    margin-top: 10px; /* 控制文字与视频之间的间距 */
                    font-size: 16px; /* 设置文字大小 */
                }
            </style>
            <div id="demotooltip"></div>
            <h4>Data Examples</h4>
            <hr>
            <div id="dataset">
                <div id="videos"></div>
                <div id="video_details"></div>

                <div class="video-container">
                    <div class="video-item">
                        <video width="300" controls>
                            <source src="./examples/video_normal_49_7_1745.mp4" type="video/mp4">
                        </video>
                        <div class="caption">Vid: 49_7_1745<br>Video duration: 166.28s<br>Action sequence:</div>
                        <img src="imgs/as_49_7_1745.png" width="100%">
                    </div>
                    <div class="video-item">
                        <video width="300" controls>
                            <source src="./examples/video_normal_0_8_1_0_3.mp4" type="video/mp4">
                        </video>
                        <div class="caption">Vid: 0_8_1_0_3<br>Video duration: 225.49s<br>Action sequence:</div>
                        <img src="imgs/as_0_8_1_0_3.png" width="100%">
                    </div>
                    <div class="video-item">
                        <video width="300" controls>
                            <source src="./examples/video_normal_0_3_1_5_2.mp4" type="video/mp4">
                        </video>
                        <div class="caption">Vid: 0_3_1_5_2<br>Video duration: 66.13s<br>Action sequence:</div>
                        <img src="imgs/as_0_3_1_5_2.png" width="100%">
                    </div>
                    <div class="video-item">
                        <video width="300" controls>
                            <source src="./examples/video_normal_3_4_1_3_4.mp4" type="video/mp4">
                        </video>
                        <div class="caption">Vid: 3_4_1_3_4<br>Video duration: 68.97s<br>Action sequence:</div>
                        <img src="imgs/as_3_4_1_3_4.png" width="100%">
                    </div>
                </div>

            </div>
        </div>

        <div id="repo" class="section">
            <h4>Data Download</h4>
            <hr>
            <div class="rows">
                <div style="border-right: solid 1px var(--gray);">
                    <p style="font-size: 24px; line-height: 28px;"><b>Data Overview</b></p>
                    
                    <!--<p><b>Question Types:</b></p>
                    <ul>
                        <li>XX Question</li>
                        <li>XX Question</li>
                        <li>XX Question</li>
                        <li>XX Question</li>
                    </ul>
                    -->
                    <p><b>96 Compositional Actions</b></p>
                    <p><b>26.2K Synthetic Video Situations</b></p>
                    <p><b>78K high-quality Temporal Annotations</b></p>
                    <p><b>Commonsense Activity Graph</b></p>
                    <p><b>Controllable Activity Manuscript</b></p>
                    <!--
                    <ul>
                        <li>111 action classes</li>
                        <li>37 entity classes</li>
                        <li>24 relationship classes</li>
                    </ul>
                    -->

                </div>
                <div style="border-right: solid 1px var(--gray); padding-right: 20px;">
                    <p style="font-size: 24px; line-height: 28px;"><b>Data Download</b></p>
                    <p>Situation Component</p>
                    <a class="download" href="https://github.com/SVLTA-Bench/SVLTA_Benchmark/tree/main/annotations/situation_component" download>component <span>txt</span></a>

                    <p>Annotations for Specific Temporal Grounding Models Evaluation</p>
                    <a class="download" href="https://github.com/SVLTA-Bench/SVLTA_Benchmark/blob/main/annotations/generation_annotations/train.json" download>train <span>json</span></a>
                    <a class="download" href="https://github.com/SVLTA-Bench/SVLTA_Benchmark/blob/main/annotations/generation_annotations/val.json" download>val <span>json</span></a>
                    <a class="download" href="https://github.com/SVLTA-Bench/SVLTA_Benchmark/blob/main/annotations/generation_annotations/test.json" download>test <span>json</span></a>
                    <p>Annotations for Video LLMs Evaluation</p>
                    <a class="download" href="https://github.com/SVLTA-Bench/SVLTA_Benchmark/blob/main/annotations/generation_annotations/SVLTA_vllm.json">anno <span>json</span></a>
                    <p>Video Features</p>
                    <a class="download" href="https://drive.google.com/file/d/1yTYXRqhSZH8CRf_jwZN3LoX4qjglLKDB/view?usp=drive_link">feature <span>npy</span></a>
                    <p>Raw Videos (the video is about 400GB, we are still trying to host it)</p>
                    <a class="download" href="https://svlta-bench.github.io/SVLTA/">TODO <span>mp4</span></a>
                    
                    <!--
                    <p>Questions, Answers and Situation Commonsense Knowledge Graphs</p>
                    <a class="download" href="SOK-Bench_train.json" download>Train <span>json</span></a>
                    <a class="download" href="SOK-Bench_val.json" download>Val <span>json</span></a>
                    <a class="download" href="SOK-Bench_test.json" download>Test <span>json</span></a><br>
                    <a class="download" href="SOK-Bench_split_file.json" download>Train/Val/Test Split File <span>json</span></a>

                    <p>Situation Video Clips</p>
                    <a class="download" href="Video_Segments.csv">Video Segments <span>csv</span></a><br>
                    <a class="download" href="Video_Keyframe_IDs.csv" download>Video Keyframe IDs <span>csv</span></a><br>
                    <a class="download" href="https://XX" download>Raw Videos <span>mp4</span></a>
                    <a class="download" href="https://XX" download>Preprocess Scripts</a>
                    
                    <p>Annotations</p>
                    <a class="download" href="Annoation_Classes.zip" download>Annotation Classes <span>zip</span></a><br>
                    
                    <p>Rationals</p>
                    <a class="download" href="QA_Rationals.json" download>Question-Answer Rationals <span>csv</span></a><br>
                    -->
                    
                    <!--
                    <p>Download from Baidu Yunpan (百度云盘)</p>
                    <a class="download" href="https://XX" download>Data Download</a> Access Code: 
                    -->
                    
                </div>
  
            </div>
            <hr>
        </div>

        <!--
        <div id="paper" class="section">
            <h4>Paper</h4>
            <img src="imgs/paper.png" width="100%">
            <a class="download" href="https://arxiv.org/abs/2405.09713" style="float: right" download>Link to Paper</a>
            <div class="citation">
                @inproceedings{SOK-Bench,
                <span>author = {Wang*, Andong and Wu*, Bo and Chen, Sunli and Chen, Zhenfang and Guan, Haotian and Lee, Wei-Ning and Li, Erran Li and Tenenbaum, Joshua B and Gan, Chuang},</span>
                <span>title = {SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge},</span>
                <span>booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},</span>
                <span>year = {2024}</span>
                }
            </div>
        </div>
        -->
        <!--
        <div id="author" class="section">
            <h4>Author</h4>
            <br>
            
            <div class="rows">
                <div class="author">
                    <img src="imgs/photos/andong.png">
                    <p><b>Andong Wang</b>*<br>The University of Hong Kong<br></p>
                </div>
                <div class="author">
                    <img src="imgs/photos/bo.png">
                    <p><b>Bo Wu</b>*<br>MIT-IBM Watson AI Lab<br></p>
                </div>
                <div class="author">
                    <img src="imgs/photos/sunli.png">
                    <p><b>Sunli Chen</b><br>Tsinghua University<br></p>
                </div>
                <div class="author">
                    <img src="imgs/photos/zhenfang.png">
                    <p><b>Zhenfang Chen</b><br>MIT-IBM Watson AI Lab<br></p>
                </div>
            </div>
            <div class="rows">
                <div class="author">
                    <img src="imgs/photos/haotian.png">
                    <p><b>Haotian Guan</b><br>The University of Hong Kong<br></p>
                </div>
                <div class="author">
                    <img src="imgs/photos/weining.jpeg">
                    <p><b>Wei-Ning Lee</b><br>The University of Hong Kong<br></p>
                </div>
                <div class="author">
                    <img src="imgs/photos/li.jpeg">
                    <p><b>Li Erran Li</b><br>AWS AI<br></p>
                </div>
                <div class="author">
                    <img src="imgs/photos/chuang.png">
                    <p><b>Chuang Gan</b><br>UMass Amherst, MIT-IBM Watson AI Lab<br></p>
                </div>
            </div>
        </div>
        -->

        <div class="footer">
            <div>
                <p>
                    Contact us: <a href="cs.dh97@gmail.com">cs.dh97@gmail.com</a>.<br>
                    The website is built by SVLTA team, and the style is designed by Lucy Yip.<br>
                    Copyright © 2024 SVLTA<br>
                    <a href="https://github.com/SVLTA-Bench/SVLTA/blob/main/LICENSE">Dataset License</a>
                </p>
            </div>
            <!--
            <div style="text-align: end">
                <a href="#">Privacy Policy</a><br>
                <a href="#">Terms of Service</a><br>
            </div>
            -->
        </div>

    </div>

    <!-- JavaScript Bundle with Popper -->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM" crossorigin="anonymous"></script>
    <script src="js/setting.js"></script>
    <script src="js/teaser.js"></script>
    <script src="js/interaction.js"></script>
    <script src="js/dataset.js"></script>

</body>
</html>
